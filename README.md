# MMLUâ€‘Pro

|[**ğŸ¤— Dataset**](https://huggingface.co/datasets/TIGER-Lab/MMLU-Pro) | [**ğŸ† Leaderboard**](https://huggingface.co/spaces/TIGER-Lab/MMLU-Pro) | [**ğŸ“– Paper**](https://arxiv.org/abs/2406.01574) |
|---|---|---|

This repo contains the evaluation code for the NeurIPSâ€‘2024 paper **â€œMMLUâ€‘Pro: A More Robust and Challenging Multiâ€‘Task Language Understanding Benchmarkâ€**.  

## Introduction

We introduce **MMLUâ€‘Pro**, an enhanced benchmark designed to evaluate languageâ€‘understanding models across broader and more challenging tasks. Building on the Massive Multitask Language Understanding (MMLU) dataset, MMLUâ€‘Pro integrates more reasoningâ€‘focused questions and expands the answer choices per question from **4â€¯â†’â€¯10**, significantly raising the difficulty and reducing the chance of success through random guessing.

MMLUâ€‘Pro comprises **â‰ˆ12â€¯k** rigorously curated questions from academic exams and textbooks, spanning **14** diverse domains (Biology, Business, Chemistry, â€¦, Others).  

Our experiments show a **16â€¯%â€“33â€¯%** drop in accuracy compared to the original MMLU and a **â‰ˆ2â€¯%** reduction in promptâ€‘sensitivity (vs. 4â€‘5â€¯% on MMLU). Moreover, Chainâ€‘ofâ€‘Thought (CoT) reasoning now *helps* rather than hurts, confirming that the benchmark contains substantially more complex reasoning questions.

<img width="1432" alt="MMLUâ€‘Pro overview" src="https://github.com/TIGER-AI-Lab/MMLU-Pro/assets/20929360/8e369fc2-5b6b-4bab-8a44-9e222e742027">

---

## Updates
- **Octâ€¯10â€¯2024** â€“ Added the 24 tested prompt styles from the paper.
- **Aprâ€¯2025** â€“ New **local inference script** for vLLM & Ollama (`vllm_ollama_eval.py`).  

---

## Dataset Creation

MMLUâ€‘Pro was created to provide language models with a more challenging and robust benchmark, pushing the boundaries of expertâ€‘level knowledge and reasoning. See the Huggingâ€¯Face hub for details: https://huggingface.co/datasets/TIGER-Lab/MMLUâ€‘Pro  

---

## Evaluation

### 1ï¸âƒ£ Local inference with **vLLM** or **Ollama**

We now ship a readyâ€‘toâ€‘run Python script that can talk to any OpenAIâ€‘compatible server (vLLM, vLLMâ€‘served on `/v1`, or an Ollama instance).  
The script lives at the repository root:



#### What it does
* Loads the MMLUâ€‘Pro testâ€‘set (and the validation set for fewâ€‘shot CoT examples).  
* Builds prompts in the â€œthink stepâ€‘byâ€‘step â†’ answerâ€ style used in the paper.  
* Sends the prompts to the configured backend (vLLM or Ollama) using the appropriate endpoint (`/chat/completions`, `/completions`, or `/api/chat`).  
* Extracts the answer (Aâ€‘J) with a set of robust regexes, records latency, and writes **perâ€‘subject JSON results** plus a **summary JSON**.  
* Supports multiâ€‘processing via `ThreadPoolExecutor` (defaultâ€¯8 workers).

#### Quick start

```bash
# 1ï¸âƒ£ Clone & cd (you already did this)
git clone https://github.com/yash-vardhansingh/MMLU-PRO.git
cd MMLU-PRO

# 2ï¸âƒ£ (Optional) Create a clean Python env
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt   # includes requests, tqdm, datasets, etc.

# 3ï¸âƒ£ Run the script
python evaluate_master.py \
    --output_dir eval_results \
    --vllm_base_url http://127.0.0.1:8000/v1 \   # <-- your vLLM server
    --model_alias llama3:8b \                  # name known to the server
    --backend auto \                           # autoâ€‘detect (vLLM vs Ollama)
    --max_cot_examples 1 \                     # how many CoT demos per subject
    --concurrency 8 \                          # parallel workers
    --temperature 0.0 \                        # deterministic
    --max_tokens 512
